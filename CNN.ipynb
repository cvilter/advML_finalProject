{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import Vectors, build_vocab_from_iterator\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "from torchmetrics.classification import BinaryRecall\n",
    "from torchmetrics.classification import BinaryPrecision\n",
    "\n",
    "import math\n",
    "from string import digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/ML_text_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"New Text Underlined\", \"\")\n",
    "    text = text.replace(\"[DELETED TEXT BRACKETED]\", \"\")\n",
    "    text = text.replace(\"HOUSE BILL\", \"\")\n",
    "    text = text.replace(\"SENATE BILL\", \"\")\n",
    "    text = re.sub(\" {2,}\", \" \", text)\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    text = text.translate(remove_digits)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text'] = df['text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2660"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = zip(df['discriminatory'], df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"New Text Underlined\", \"\")\n",
    "    text = text.replace(\"[DELETED TEXT BRACKETED]\", \"\")\n",
    "    text = re.sub(\" {2,}\", \" \", text)\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    text = text.translate(remove_digits)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        text = clean_text(text)\n",
    "        text = tokenizer(text)\n",
    "        yield text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(data), specials=[\"<unk>\"], min_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3985\n",
      "<unk>\n",
      "is\n",
      "103\n",
      "182\n",
      "2496\n",
      "3617\n"
     ]
    }
   ],
   "source": [
    "# Checking the result\n",
    "print(len(vocab))\n",
    "print(vocab.lookup_token(0))\n",
    "print(vocab.lookup_token(15))\n",
    "print(vocab[\"bill\"])\n",
    "print(vocab[\"gender\"])\n",
    "print(vocab[\"pronoun\"])\n",
    "print(vocab[\"transgender\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " '  Enrolled HB  LAWS OF ALASKA            Source Chapter No.  CSHB (FIN) _______        AN ACT    Making supplemental appropriati ons and other appropriations; ma king appropriations under  art. IX, sec. (c), Constitution of the State of Alaska, from the constitutional budget reserve  fund; and providing for  an effective date.     _______________     BE IT ENACTED BY THE LEGISLATURE OF THE STATE OF ALASKA:      THE ACT FOLLOWS ON PAGE    -- Enrolled HB                        AN ACT     Making supplemental appropriati ons and other appropriations; ma king appropriations under   art. IX, sec. (c), Constitution of the State of Alaska, from the constitutional budget reserve   fund; and providing for  an effective date.   _______________   (SECTION  OF THIS ACT BEGINS ON PAGE )      CSHB (FIN), Sec.      -  -     * Section . The following appropriation items  are for operating expenditure s from the   general fund or other funds as set out in the fiscal year  budget summary for the   operating budget by funding source to the agencies named for th e purposes expressed for the   fiscal year beginning July ,  and ending June , , un less otherwise indicated.           A p p r o p r i a t i o n  G e n e r a l  O t h e r           A l l o c a t i o n s  I t e m s  F unds Funds   * * * * *              * * * * *   * * * * * Department of Health * * * * *   * * * * *              * * * * *    Public Assistance  ,, ,, ,,    Public Assistance Field ,,     Services   Medicaid Services  -,, -,,     Medicaid Services -,,   (SECTION  OF THIS ACT BEGINS ON THE NEXT PAGE)         CSHB (FIN), Sec.    -  -     * Sec. . The following sets out the funding  by agency for the appropriat ions made in sec.  of   this Act.    Funding Source    Amount   Department of Health       Federal Receipts ,,    *** Total Agency Funding *** ,,   * * * * * Total Budget * * * * * ,,    (SECTION  OF THIS ACT BEGINS ON THE NEXT PAGE)      CSHB (FIN), Sec.      -  -     * Sec. . The following sets out the state wide funding for the appropriat ions made in sec.  of   this Act.    Funding Source    Amount   Federal Receipts       Federal Receipts ,,    *** Total Federal Receipts *** ,,   (SECTION  OF THIS ACT BEGINS ON THE NEXT PAGE)     -- Enrolled HB     * Sec. . DEPARTMENT OF ADMINISTRATION.  (a) The sum of $,, is   appropriated from the general f und to the Department of Adminis tration, legal and advocacy   services, office of public advocacy, for the cost of implementi ng ch. , SLA , for the   fiscal year ending June , .    (b)  The sum of $, is appropr iated from the general fund to the Department of   Administration, legal and advocac y services, office of public a dvocacy, to address case   backlogs for the fiscal y ears ending June , , and June  , .   (c)  The sum of $, is appropr iated from the general fund to the Department of   Administration, legal and advocac y services, public defender ag ency, for the cost of   implementing ch. , SLA , for the  fiscal year ending June , .   (d)  The sum of $, is appropr iated from the general fund to the Department of   Administration, legal and advocac y services, public defender ag ency, to address case   backlogs for the fiscal y ears ending June , , and June  , .      * Sec. . CONSTITUTIONAL BUDGET RESERVE FUND. (a) If, after the appropria tion   made in sec. (b), ch. , SLA , the unrestricted state re venue available for   appropriation in fiscal year  is insufficient to cover the general fund appr opriations that   take effect in fiscal year  that are made in ch. , SLA  , and the general fund   appropriations that take effect i n fiscal year  that are ma de in ch. , SLA , not   including the appropriation made  in sec. (u), ch. , SLA  , the amount necessary to   balance revenue and general f und appropriations t hat take effec t in fiscal year  that are   made in ch. , SLA , and the general fund appropriations t hat take effect in fiscal year    that are made in ch. , SLA , not including the appro priation made in sec. (u),   ch. , SLA , is appropriated to the general fund from the budget reserve fund (art. IX,   sec. , Constitution of the  State of Alaska).   (b)  If, after the appropriation made in (a) of this section, t he unrestricted state revenue   available for appropriation in  fiscal year  is insufficient  to cover the general fund   appropriations that take effect  in fiscal year , not includ ing the appropriation made in sec.   (u), ch. , SLA , the amount necessary to balance revenu e and general fund   appropriations that take effect  in fiscal year , not includ ing the appropriation made in sec.   (u), ch. , SLA , and not to exceed $,,, is app ropriated to the general fund   from the budget reserve fund (art. IX, sec. , Constitution of  the State of Alaska).    Enrolled HB  --  (c)  The appropriations made from the budget reserve fund (art.  IX, sec. ,   Constitution of the State of Alaska) in (a) and (b) of this sec tion are made under art. IX, sec.   (c), Constitution of the  State of Alaska.      * Sec. . This Act takes effect imme diately under AS ..(c).  ')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#truncate bills\n",
    "data = zip(df['discriminatory'], df['text'])\n",
    "data_list = [(label, tokens) for (label, tokens) in data] \n",
    "data_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize bill length - truncate and pad\n",
    "for i, (label, text) in enumerate(data_list):\n",
    "    text = clean_text(text)\n",
    "    tokens = tokenizer(text)\n",
    "    if len(tokens) > 500:\n",
    "        tokens = tokens[:500]\n",
    "        data_list[i] = (label, tokens)\n",
    "    else:\n",
    "        pad_len = 500 - len(tokens)\n",
    "        padding = ['<unk>'] * pad_len\n",
    "        tokens = tokens + padding\n",
    "        data_list[i] = (label, tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    labels = torch.tensor([x[0] for x in batch])\n",
    "    words = [x[1] for x in batch]\n",
    "    word_idxs = []\n",
    "    for window in words:\n",
    "        window_idxs = [vocab[word] for word in window]\n",
    "        word_idxs.append(window_idxs)\n",
    "    word_idxs = torch.tensor(word_idxs)\n",
    "    return labels, word_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.ModuleList):\n",
    "\n",
    "   def __init__(self,\n",
    "                 embedding_dim,\n",
    "                 sequence_len,\n",
    "                 vocab_len, \n",
    "                 out_size,\n",
    "                 stride):       \n",
    "      super(CNNClassifier, self).__init__()\n",
    "\n",
    "      self.embedding_size = embedding_dim\n",
    "      self.seq_len = sequence_len\n",
    "      self.vocab_len = vocab_len\n",
    "  \n",
    "      # Dropout definition\n",
    "      self.dropout = nn.Dropout(0.25)\n",
    "      \n",
    "      # CNN parameters definition\n",
    "      # Kernel sizes\n",
    "      self.kernel_1 = 2\n",
    "      self.kernel_2 = 3\n",
    "      self.kernel_3 = 4\n",
    "      self.kernel_4 = 5\n",
    "      \n",
    "      # Output size for each convolution\n",
    "      self.out_size = out_size\n",
    "      # Number of strides for each convolution\n",
    "      self.stride = stride\n",
    "      \n",
    "      # Embedding layer definition\n",
    "      self.embedding = nn.Embedding(self.vocab_len + 1, self.embedding_size, padding_idx=0)\n",
    "      \n",
    "      # Convolution layers definition\n",
    "      self.conv_1 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_1, self.stride)\n",
    "      self.conv_2 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_2, self.stride)\n",
    "      self.conv_3 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_3, self.stride)\n",
    "      self.conv_4 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_4, self.stride)\n",
    "      \n",
    "      # Max pooling layers definition\n",
    "      self.pool_1 = nn.MaxPool1d(self.kernel_1, self.stride)\n",
    "      self.pool_2 = nn.MaxPool1d(self.kernel_2, self.stride)\n",
    "      self.pool_3 = nn.MaxPool1d(self.kernel_3, self.stride)\n",
    "      self.pool_4 = nn.MaxPool1d(self.kernel_4, self.stride)\n",
    "      \n",
    "      # Fully connected layer definition\n",
    "      self.fc = nn.Linear(self.in_features_fc(), 1)\n",
    "\n",
    "   def in_features_fc(self):\n",
    "      '''Calculates the number of output features after Convolution + Max pooling\n",
    "         \n",
    "      Convolved_Features = ((embedding_size + (2 * padding) - dilation * (kernel - 1) - 1) / stride) + 1\n",
    "      Pooled_Features = ((embedding_size + (2 * padding) - dilation * (kernel - 1) - 1) / stride) + 1\n",
    "      \n",
    "      source: https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n",
    "      '''\n",
    "      # Calcualte size of convolved/pooled features for convolution_1/max_pooling_1 features\n",
    "      out_conv_1 = ((self.embedding_size - 1 * (self.kernel_1 - 1) - 1) / self.stride) + 1\n",
    "      out_conv_1 = math.floor(out_conv_1)\n",
    "      out_pool_1 = ((out_conv_1 - 1 * (self.kernel_1 - 1) - 1) / self.stride) + 1\n",
    "      out_pool_1 = math.floor(out_pool_1)\n",
    "      \n",
    "      # Calcualte size of convolved/pooled features for convolution_2/max_pooling_2 features\n",
    "      out_conv_2 = ((self.embedding_size - 1 * (self.kernel_2 - 1) - 1) / self.stride) + 1\n",
    "      out_conv_2 = math.floor(out_conv_2)\n",
    "      out_pool_2 = ((out_conv_2 - 1 * (self.kernel_2 - 1) - 1) / self.stride) + 1\n",
    "      out_pool_2 = math.floor(out_pool_2)\n",
    "      \n",
    "      # Calcualte size of convolved/pooled features for convolution_3/max_pooling_3 features\n",
    "      out_conv_3 = ((self.embedding_size - 1 * (self.kernel_3 - 1) - 1) / self.stride) + 1\n",
    "      out_conv_3 = math.floor(out_conv_3)\n",
    "      out_pool_3 = ((out_conv_3 - 1 * (self.kernel_3 - 1) - 1) / self.stride) + 1\n",
    "      out_pool_3 = math.floor(out_pool_3)\n",
    "      \n",
    "      # Calcualte size of convolved/pooled features for convolution_4/max_pooling_4 features\n",
    "      out_conv_4 = ((self.embedding_size - 1 * (self.kernel_4 - 1) - 1) / self.stride) + 1\n",
    "      out_conv_4 = math.floor(out_conv_4)\n",
    "      out_pool_4 = ((out_conv_4 - 1 * (self.kernel_4 - 1) - 1) / self.stride) + 1\n",
    "      out_pool_4 = math.floor(out_pool_4)\n",
    "      \n",
    "      # Returns \"flattened\" vector (input for fully connected layer)\n",
    "      return (out_pool_1 + out_pool_2 + out_pool_3 + out_pool_4) * self.out_size\n",
    "   \n",
    "   def forward(self, x):\n",
    "\n",
    "      # Sequence of tokes is filterd through an embedding layer\n",
    "      x = self.embedding(x)\n",
    "      \n",
    "      # Convolution layer 1 is applied\n",
    "      x1 = self.conv_1(x)\n",
    "      x1 = torch.relu(x1)\n",
    "      x1 = self.pool_1(x1)\n",
    "      \n",
    "      # Convolution layer 2 is applied\n",
    "      x2 = self.conv_2(x)\n",
    "      x2 = torch.relu((x2))\n",
    "      x2 = self.pool_2(x2)\n",
    "   \n",
    "      # Convolution layer 3 is applied\n",
    "      x3 = self.conv_3(x)\n",
    "      x3 = torch.relu(x3)\n",
    "      x3 = self.pool_3(x3)\n",
    "      \n",
    "      # Convolution layer 4 is applied\n",
    "      x4 = self.conv_4(x)\n",
    "      x4 = torch.relu(x4)\n",
    "      x4 = self.pool_4(x4)\n",
    "      \n",
    "      # The output of each convolutional layer is concatenated into a unique vector\n",
    "      union = torch.cat((x1, x2, x3, x4), 2)\n",
    "      union = union.reshape(union.size(0), -1)\n",
    "\n",
    "      # The \"flattened\" vector is passed through a fully connected layer\n",
    "      out = self.fc(union)\n",
    "      # Dropout is applied\t\t\n",
    "      out = self.dropout(out)\n",
    "      # Activation function is applied\n",
    "      out = torch.sigmoid(out)\n",
    "      \n",
    "      return out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNClassifier(embedding_dim = 64,\n",
    "                 sequence_len = 500,\n",
    "                 vocab_len = len(vocab), \n",
    "                 out_size = 32,\n",
    "                 stride = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_an_epoch(dataloader, optimizer):\n",
    "    model.train() \n",
    "    log_interval = 10\n",
    "\n",
    "    for idx, (label, text) in enumerate(dataloader):\n",
    "        model.zero_grad()\n",
    "        log_probs = model(text)\n",
    "        loss = loss_function(log_probs, label.type('torch.FloatTensor'))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            print(f'At iteration {idx} the loss is {loss:.3f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():    \n",
    "        total_acc, total_count = 0, 0\n",
    "        all_recall = []\n",
    "        all_precision = []\n",
    "        recall_metric = BinaryRecall()\n",
    "        precision_metric = BinaryPrecision()\n",
    "        for idx, (label, word_idxs) in enumerate(dataloader):\n",
    "            log_probs = model(word_idxs)\n",
    "            total_acc += (log_probs.round() == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "            recall = recall_metric(log_probs, label)\n",
    "            precision = precision_metric(log_probs, label)\n",
    "            all_recall.append(recall)\n",
    "            all_precision.append(precision)\n",
    "    accuracy = total_acc/total_count\n",
    "\n",
    "    return accuracy, np.mean(all_recall), np.mean(all_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2128 train 266 validation 266 testing\n"
     ]
    }
   ],
   "source": [
    "# Prep for data split\n",
    "len_train = int(len(data_list) * .8)\n",
    "len_valid = int((len(data_list) - len_train)/2)\n",
    "len_test = len(data_list) - len_train - len_valid\n",
    "print(len_train, \"train\", len_valid, \"validation\", len_test, \"testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "train_data, valid_data, test_data = random_split(\n",
    "    data_list, [len_train, len_valid, len_test])\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, \n",
    "                              collate_fn=collate_fn)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=BATCH_SIZE,\n",
    "                              shuffle=False, \n",
    "                              collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
    "                             shuffle=False, \n",
    "                             collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 10 the loss is 13.679.\n",
      "At iteration 20 the loss is 13.720.\n",
      "At iteration 30 the loss is 13.861.\n",
      "At iteration 40 the loss is 16.783.\n",
      "At iteration 50 the loss is 16.877.\n",
      "At iteration 60 the loss is 8.026.\n",
      "At iteration 70 the loss is 22.114.\n",
      "At iteration 80 the loss is 19.421.\n",
      "At iteration 90 the loss is 8.204.\n",
      "At iteration 100 the loss is 5.420.\n",
      "At iteration 110 the loss is -0.000.\n",
      "At iteration 120 the loss is 8.310.\n",
      "At iteration 130 the loss is 19.042.\n",
      "Epoch: 1, time taken: 4.4s, validation accuracy: 0.436, recall: 0.9043417572975159, precision: 0.344294935464859.\n",
      "At iteration 10 the loss is 10.543.\n",
      "At iteration 20 the loss is 10.623.\n",
      "At iteration 30 the loss is 15.980.\n",
      "At iteration 40 the loss is 13.382.\n",
      "At iteration 50 the loss is 5.265.\n",
      "At iteration 60 the loss is 5.253.\n",
      "At iteration 70 the loss is 10.689.\n",
      "At iteration 80 the loss is 5.399.\n",
      "At iteration 90 the loss is 7.888.\n",
      "At iteration 100 the loss is 10.666.\n",
      "At iteration 110 the loss is 10.924.\n",
      "At iteration 120 the loss is 18.854.\n",
      "At iteration 130 the loss is 13.463.\n",
      "Epoch: 2, time taken: 4.2s, validation accuracy: 0.752, recall: 0.45098042488098145, precision: 0.5841736793518066.\n",
      "At iteration 10 the loss is 15.736.\n",
      "At iteration 20 the loss is 10.446.\n",
      "At iteration 30 the loss is 7.493.\n",
      "At iteration 40 the loss is 18.416.\n",
      "At iteration 50 the loss is 10.132.\n",
      "At iteration 60 the loss is 5.038.\n",
      "At iteration 70 the loss is 12.789.\n",
      "At iteration 80 the loss is 10.677.\n",
      "At iteration 90 the loss is 21.439.\n",
      "At iteration 100 the loss is 9.922.\n",
      "At iteration 110 the loss is 12.785.\n",
      "At iteration 120 the loss is 10.004.\n",
      "At iteration 130 the loss is 10.173.\n",
      "Epoch: 3, time taken: 4.2s, validation accuracy: 0.741, recall: 0.3180672228336334, precision: 0.5588235259056091.\n",
      "At iteration 10 the loss is 15.162.\n",
      "At iteration 20 the loss is 12.745.\n",
      "At iteration 30 the loss is 5.224.\n",
      "At iteration 40 the loss is 2.331.\n",
      "At iteration 50 the loss is 12.617.\n",
      "At iteration 60 the loss is 12.936.\n",
      "At iteration 70 the loss is 18.271.\n",
      "At iteration 80 the loss is 4.997.\n",
      "At iteration 90 the loss is 9.920.\n",
      "At iteration 100 the loss is 7.628.\n",
      "At iteration 110 the loss is 7.320.\n",
      "At iteration 120 the loss is 6.852.\n",
      "At iteration 130 the loss is 20.430.\n",
      "Epoch: 4, time taken: 4.4s, validation accuracy: 0.737, recall: 0.2488795518875122, precision: 0.5882353186607361.\n",
      "At iteration 10 the loss is 15.492.\n",
      "At iteration 20 the loss is 17.997.\n",
      "At iteration 30 the loss is 9.660.\n",
      "At iteration 40 the loss is 9.940.\n",
      "At iteration 50 the loss is 17.774.\n",
      "At iteration 60 the loss is 12.147.\n",
      "At iteration 70 the loss is 6.888.\n",
      "At iteration 80 the loss is 7.163.\n",
      "At iteration 90 the loss is 15.270.\n",
      "At iteration 100 the loss is 7.119.\n",
      "At iteration 110 the loss is 20.807.\n",
      "At iteration 120 the loss is 12.516.\n",
      "At iteration 130 the loss is 9.834.\n",
      "Epoch: 5, time taken: 4.2s, validation accuracy: 0.748, recall: 0.3558823764324188, precision: 0.6018207669258118.\n",
      "At iteration 10 the loss is 4.609.\n",
      "At iteration 20 the loss is 4.489.\n",
      "At iteration 30 the loss is 9.551.\n",
      "At iteration 40 the loss is 20.824.\n",
      "At iteration 50 the loss is 9.715.\n",
      "At iteration 60 the loss is 17.839.\n",
      "At iteration 70 the loss is 17.488.\n",
      "At iteration 80 the loss is 12.708.\n",
      "At iteration 90 the loss is 2.566.\n",
      "At iteration 100 the loss is 2.537.\n",
      "At iteration 110 the loss is 14.466.\n",
      "At iteration 120 the loss is 20.744.\n",
      "At iteration 130 the loss is 2.303.\n",
      "Epoch: 6, time taken: 4.2s, validation accuracy: 0.774, recall: 0.47843137383461, precision: 0.6704481840133667.\n",
      "At iteration 10 the loss is 17.175.\n",
      "At iteration 20 the loss is 6.528.\n",
      "At iteration 30 the loss is 9.027.\n",
      "At iteration 40 the loss is 9.461.\n",
      "At iteration 50 the loss is 14.537.\n",
      "At iteration 60 the loss is 12.313.\n",
      "At iteration 70 the loss is 23.005.\n",
      "At iteration 80 the loss is 20.060.\n",
      "At iteration 90 the loss is 15.519.\n",
      "At iteration 100 the loss is 9.552.\n",
      "At iteration 110 the loss is 4.869.\n",
      "At iteration 120 the loss is 11.977.\n",
      "At iteration 130 the loss is 14.533.\n",
      "Epoch: 7, time taken: 4.2s, validation accuracy: 0.763, recall: 0.3970588147640228, precision: 0.6998599767684937.\n",
      "At iteration 10 the loss is 6.986.\n",
      "At iteration 20 the loss is 7.059.\n",
      "At iteration 30 the loss is 12.330.\n",
      "At iteration 40 the loss is 11.890.\n",
      "At iteration 50 the loss is 11.550.\n",
      "At iteration 60 the loss is 17.570.\n",
      "At iteration 70 the loss is 11.592.\n",
      "At iteration 80 the loss is 9.096.\n",
      "At iteration 90 the loss is 12.460.\n",
      "At iteration 100 the loss is 9.240.\n",
      "At iteration 110 the loss is 9.633.\n",
      "At iteration 120 the loss is 11.732.\n",
      "At iteration 130 the loss is 4.262.\n",
      "Epoch: 8, time taken: 4.1s, validation accuracy: 0.759, recall: 0.34061622619628906, precision: 0.6725490093231201.\n",
      "At iteration 10 the loss is 17.378.\n",
      "At iteration 20 the loss is 14.677.\n",
      "At iteration 30 the loss is 11.441.\n",
      "At iteration 40 the loss is 6.821.\n",
      "At iteration 50 the loss is 9.338.\n",
      "At iteration 60 the loss is 14.181.\n",
      "At iteration 70 the loss is 12.046.\n",
      "At iteration 80 the loss is 4.532.\n",
      "At iteration 90 the loss is 9.671.\n",
      "At iteration 100 the loss is 9.968.\n",
      "At iteration 110 the loss is 4.279.\n",
      "At iteration 120 the loss is 11.799.\n",
      "At iteration 130 the loss is 6.901.\n",
      "Epoch: 9, time taken: 4.4s, validation accuracy: 0.771, recall: 0.388655424118042, precision: 0.7362744808197021.\n",
      "At iteration 10 the loss is 4.690.\n",
      "At iteration 20 the loss is 9.181.\n",
      "At iteration 30 the loss is 11.721.\n",
      "At iteration 40 the loss is 12.308.\n",
      "At iteration 50 the loss is 17.401.\n",
      "At iteration 60 the loss is 11.772.\n",
      "At iteration 70 the loss is 9.440.\n",
      "At iteration 80 the loss is 4.275.\n",
      "At iteration 90 the loss is 6.987.\n",
      "At iteration 100 the loss is 8.734.\n",
      "At iteration 110 the loss is 11.259.\n",
      "At iteration 120 the loss is 17.747.\n",
      "At iteration 130 the loss is 11.418.\n",
      "Epoch: 10, time taken: 4.1s, validation accuracy: 0.752, recall: 0.3014005720615387, precision: 0.6235294342041016.\n",
      "At iteration 10 the loss is 4.213.\n",
      "At iteration 20 the loss is 14.976.\n",
      "At iteration 30 the loss is 9.204.\n",
      "At iteration 40 the loss is 9.705.\n",
      "At iteration 50 the loss is 11.603.\n",
      "At iteration 60 the loss is 17.423.\n",
      "At iteration 70 the loss is 11.350.\n",
      "At iteration 80 the loss is 9.040.\n",
      "At iteration 90 the loss is 9.440.\n",
      "At iteration 100 the loss is 17.035.\n",
      "At iteration 110 the loss is 11.232.\n",
      "At iteration 120 the loss is 14.812.\n",
      "At iteration 130 the loss is 14.281.\n",
      "Epoch: 11, time taken: 4.0s, validation accuracy: 0.759, recall: 0.3523809611797333, precision: 0.7313725352287292.\n",
      "At iteration 10 the loss is 14.891.\n",
      "At iteration 20 the loss is 14.406.\n",
      "At iteration 30 the loss is 9.407.\n",
      "At iteration 40 the loss is 9.299.\n",
      "At iteration 50 the loss is 11.863.\n",
      "At iteration 60 the loss is 4.112.\n",
      "At iteration 70 the loss is 17.677.\n",
      "At iteration 80 the loss is 8.900.\n",
      "At iteration 90 the loss is 11.916.\n",
      "At iteration 100 the loss is 14.618.\n",
      "At iteration 110 the loss is 4.550.\n",
      "At iteration 120 the loss is 14.521.\n",
      "At iteration 130 the loss is 12.015.\n",
      "Epoch: 12, time taken: 4.0s, validation accuracy: 0.767, recall: 0.3523809611797333, precision: 0.7509803771972656.\n",
      "At iteration 10 the loss is 22.840.\n",
      "At iteration 20 the loss is 14.854.\n",
      "At iteration 30 the loss is 6.899.\n",
      "At iteration 40 the loss is 9.083.\n",
      "At iteration 50 the loss is 14.630.\n",
      "At iteration 60 the loss is 6.386.\n",
      "At iteration 70 the loss is 4.195.\n",
      "At iteration 80 the loss is -0.000.\n",
      "At iteration 90 the loss is 11.552.\n",
      "At iteration 100 the loss is 17.101.\n",
      "At iteration 110 the loss is 13.964.\n",
      "At iteration 120 the loss is 6.376.\n",
      "At iteration 130 the loss is 11.275.\n",
      "Epoch: 13, time taken: 4.1s, validation accuracy: 0.767, recall: 0.3523809611797333, precision: 0.7509803771972656.\n",
      "At iteration 10 the loss is 14.059.\n",
      "At iteration 20 the loss is 14.706.\n",
      "At iteration 30 the loss is 17.720.\n",
      "At iteration 40 the loss is 14.504.\n",
      "At iteration 50 the loss is 11.445.\n",
      "At iteration 60 the loss is 14.355.\n",
      "At iteration 70 the loss is 4.261.\n",
      "At iteration 80 the loss is 12.147.\n",
      "At iteration 90 the loss is 9.097.\n",
      "At iteration 100 the loss is 6.282.\n",
      "At iteration 110 the loss is 14.846.\n",
      "At iteration 120 the loss is 14.209.\n",
      "At iteration 130 the loss is 14.349.\n",
      "Epoch: 14, time taken: 4.4s, validation accuracy: 0.763, recall: 0.3523809611797333, precision: 0.7509803771972656.\n",
      "At iteration 10 the loss is 9.193.\n",
      "At iteration 20 the loss is 19.764.\n",
      "At iteration 30 the loss is 16.923.\n",
      "At iteration 40 the loss is 16.716.\n",
      "At iteration 50 the loss is 4.603.\n",
      "At iteration 60 the loss is 19.953.\n",
      "At iteration 70 the loss is 14.773.\n",
      "At iteration 80 the loss is 4.150.\n",
      "At iteration 90 the loss is 8.776.\n",
      "At iteration 100 the loss is 8.776.\n",
      "At iteration 110 the loss is 11.586.\n",
      "At iteration 120 the loss is 14.448.\n",
      "At iteration 130 the loss is 9.230.\n",
      "Epoch: 15, time taken: 4.4s, validation accuracy: 0.759, recall: 0.3523809611797333, precision: 0.7453781366348267.\n",
      "At iteration 10 the loss is 6.881.\n",
      "At iteration 20 the loss is 20.310.\n",
      "At iteration 30 the loss is 14.997.\n",
      "At iteration 40 the loss is 4.658.\n",
      "At iteration 50 the loss is 11.730.\n",
      "At iteration 60 the loss is 17.524.\n",
      "At iteration 70 the loss is 20.688.\n",
      "At iteration 80 the loss is 9.101.\n",
      "At iteration 90 the loss is 19.583.\n",
      "At iteration 100 the loss is 14.106.\n",
      "At iteration 110 the loss is 17.226.\n",
      "At iteration 120 the loss is 4.260.\n",
      "At iteration 130 the loss is 11.295.\n",
      "Epoch: 16, time taken: 3.5s, validation accuracy: 0.759, recall: 0.3523809611797333, precision: 0.7453781366348267.\n",
      "At iteration 10 the loss is 4.598.\n",
      "At iteration 20 the loss is 14.050.\n",
      "At iteration 30 the loss is 11.713.\n",
      "At iteration 40 the loss is 14.632.\n",
      "At iteration 50 the loss is 17.375.\n",
      "At iteration 60 the loss is 4.311.\n",
      "At iteration 70 the loss is 17.466.\n",
      "At iteration 80 the loss is 4.204.\n",
      "At iteration 90 the loss is 4.268.\n",
      "At iteration 100 the loss is 9.079.\n",
      "At iteration 110 the loss is 11.867.\n",
      "At iteration 120 the loss is 4.470.\n",
      "At iteration 130 the loss is 9.298.\n",
      "Epoch: 17, time taken: 3.2s, validation accuracy: 0.759, recall: 0.3523809611797333, precision: 0.7453781366348267.\n",
      "At iteration 10 the loss is 12.398.\n",
      "At iteration 20 the loss is 17.585.\n",
      "At iteration 30 the loss is 7.247.\n",
      "At iteration 40 the loss is 13.910.\n",
      "At iteration 50 the loss is 14.285.\n",
      "At iteration 60 the loss is 11.708.\n",
      "At iteration 70 the loss is 6.876.\n",
      "At iteration 80 the loss is 16.753.\n",
      "At iteration 90 the loss is 7.324.\n",
      "At iteration 100 the loss is 11.778.\n",
      "At iteration 110 the loss is 6.348.\n",
      "At iteration 120 the loss is 11.310.\n",
      "At iteration 130 the loss is 13.789.\n",
      "Epoch: 18, time taken: 3.1s, validation accuracy: 0.759, recall: 0.3376750648021698, precision: 0.6865546107292175.\n",
      "At iteration 10 the loss is 9.382.\n",
      "At iteration 20 the loss is 19.744.\n",
      "At iteration 30 the loss is 4.538.\n",
      "At iteration 40 the loss is 17.690.\n",
      "At iteration 50 the loss is 17.126.\n",
      "At iteration 60 the loss is 6.789.\n",
      "At iteration 70 the loss is 12.144.\n",
      "At iteration 80 the loss is 9.297.\n",
      "At iteration 90 the loss is 11.724.\n",
      "At iteration 100 the loss is 9.608.\n",
      "At iteration 110 the loss is 9.086.\n",
      "At iteration 120 the loss is 11.823.\n",
      "At iteration 130 the loss is 9.101.\n",
      "Epoch: 19, time taken: 2.9s, validation accuracy: 0.759, recall: 0.3376750648021698, precision: 0.6865546107292175.\n",
      "At iteration 10 the loss is 11.899.\n",
      "At iteration 20 the loss is 6.514.\n",
      "At iteration 30 the loss is 9.186.\n",
      "At iteration 40 the loss is 14.149.\n",
      "At iteration 50 the loss is 14.041.\n",
      "At iteration 60 the loss is 9.609.\n",
      "At iteration 70 the loss is 16.991.\n",
      "At iteration 80 the loss is 6.432.\n",
      "At iteration 90 the loss is 17.455.\n",
      "At iteration 100 the loss is 9.290.\n",
      "At iteration 110 the loss is 4.591.\n",
      "At iteration 120 the loss is 26.227.\n",
      "At iteration 130 the loss is 12.266.\n",
      "Epoch: 20, time taken: 2.9s, validation accuracy: 0.759, recall: 0.3376750648021698, precision: 0.6865546107292175.\n",
      "At iteration 10 the loss is 7.145.\n",
      "At iteration 20 the loss is 11.569.\n",
      "At iteration 30 the loss is 14.737.\n",
      "At iteration 40 the loss is 15.148.\n",
      "At iteration 50 the loss is 13.921.\n",
      "At iteration 60 the loss is 19.890.\n",
      "At iteration 70 the loss is 11.827.\n",
      "At iteration 80 the loss is 9.715.\n",
      "At iteration 90 the loss is 7.041.\n",
      "At iteration 100 the loss is 20.499.\n",
      "At iteration 110 the loss is 19.557.\n",
      "At iteration 120 the loss is 9.072.\n",
      "At iteration 130 the loss is 13.778.\n",
      "Epoch: 21, time taken: 2.8s, validation accuracy: 0.756, recall: 0.3376750648021698, precision: 0.6865546107292175.\n",
      "At iteration 10 the loss is 14.675.\n",
      "At iteration 20 the loss is 17.294.\n",
      "At iteration 30 the loss is 14.730.\n",
      "At iteration 40 the loss is 11.991.\n",
      "At iteration 50 the loss is 12.110.\n",
      "At iteration 60 the loss is 14.676.\n",
      "At iteration 70 the loss is 14.738.\n",
      "At iteration 80 the loss is 28.751.\n",
      "At iteration 90 the loss is 17.519.\n",
      "At iteration 100 the loss is 22.986.\n",
      "At iteration 110 the loss is 11.460.\n",
      "At iteration 120 the loss is 16.669.\n",
      "At iteration 130 the loss is 14.517.\n",
      "Epoch: 22, time taken: 2.8s, validation accuracy: 0.759, recall: 0.3376750648021698, precision: 0.6865546107292175.\n",
      "At iteration 10 the loss is 6.515.\n",
      "At iteration 20 the loss is 6.782.\n",
      "At iteration 30 the loss is 4.152.\n",
      "At iteration 40 the loss is 11.856.\n",
      "At iteration 50 the loss is 17.316.\n",
      "At iteration 60 the loss is 14.433.\n",
      "At iteration 70 the loss is 9.916.\n",
      "At iteration 80 the loss is 9.495.\n",
      "At iteration 90 the loss is 14.040.\n",
      "At iteration 100 the loss is 11.696.\n",
      "At iteration 110 the loss is 11.819.\n",
      "At iteration 120 the loss is 9.073.\n",
      "At iteration 130 the loss is 20.117.\n",
      "Epoch: 23, time taken: 2.8s, validation accuracy: 0.752, recall: 0.3161064684391022, precision: 0.6277310848236084.\n",
      "At iteration 10 the loss is 11.444.\n",
      "At iteration 20 the loss is 6.590.\n",
      "At iteration 30 the loss is 9.073.\n",
      "At iteration 40 the loss is 14.522.\n",
      "At iteration 50 the loss is 6.869.\n",
      "At iteration 60 the loss is 9.176.\n",
      "At iteration 70 the loss is 4.251.\n",
      "At iteration 80 the loss is 12.110.\n",
      "At iteration 90 the loss is 11.866.\n",
      "At iteration 100 the loss is 11.401.\n",
      "At iteration 110 the loss is 7.225.\n",
      "At iteration 120 the loss is 14.739.\n",
      "At iteration 130 the loss is 14.433.\n",
      "Epoch: 24, time taken: 3.0s, validation accuracy: 0.756, recall: 0.3376750648021698, precision: 0.6865546107292175.\n",
      "At iteration 10 the loss is 6.819.\n",
      "At iteration 20 the loss is 8.640.\n",
      "At iteration 30 the loss is 9.285.\n",
      "At iteration 40 the loss is 11.398.\n",
      "At iteration 50 the loss is 11.971.\n",
      "At iteration 60 the loss is 17.293.\n",
      "At iteration 70 the loss is 11.821.\n",
      "At iteration 80 the loss is 14.180.\n",
      "At iteration 90 the loss is 4.131.\n",
      "At iteration 100 the loss is 6.432.\n",
      "At iteration 110 the loss is 4.526.\n",
      "At iteration 120 the loss is 8.988.\n",
      "At iteration 130 the loss is 11.149.\n",
      "Epoch: 25, time taken: 4.4s, validation accuracy: 0.756, recall: 0.3376750648021698, precision: 0.6865546107292175.\n",
      "At iteration 10 the loss is 14.600.\n",
      "At iteration 20 the loss is 9.389.\n",
      "At iteration 30 the loss is 14.177.\n",
      "At iteration 40 the loss is 16.894.\n",
      "At iteration 50 the loss is 8.853.\n",
      "At iteration 60 the loss is 19.929.\n",
      "At iteration 70 the loss is 14.727.\n",
      "At iteration 80 the loss is 14.729.\n",
      "At iteration 90 the loss is 9.605.\n",
      "At iteration 100 the loss is 22.873.\n",
      "At iteration 110 the loss is 11.712.\n",
      "At iteration 120 the loss is 14.054.\n",
      "At iteration 130 the loss is 14.511.\n",
      "Epoch: 26, time taken: 5.0s, validation accuracy: 0.756, recall: 0.3376750648021698, precision: 0.6865546107292175.\n",
      "At iteration 10 the loss is 19.928.\n",
      "At iteration 20 the loss is 20.480.\n",
      "At iteration 30 the loss is 9.489.\n",
      "At iteration 40 the loss is 6.868.\n",
      "At iteration 50 the loss is 8.745.\n",
      "At iteration 60 the loss is 9.493.\n",
      "At iteration 70 the loss is 9.295.\n",
      "At iteration 80 the loss is 4.188.\n",
      "At iteration 90 the loss is 11.278.\n",
      "At iteration 100 the loss is 9.175.\n",
      "At iteration 110 the loss is 11.709.\n",
      "At iteration 120 the loss is 8.643.\n",
      "At iteration 130 the loss is 7.131.\n",
      "Epoch: 27, time taken: 4.8s, validation accuracy: 0.756, recall: 0.3278711438179016, precision: 0.6865546107292175.\n",
      "At iteration 10 the loss is 4.461.\n",
      "At iteration 20 the loss is 20.494.\n",
      "At iteration 30 the loss is 11.976.\n",
      "At iteration 40 the loss is 14.468.\n",
      "At iteration 50 the loss is 4.399.\n",
      "At iteration 60 the loss is 11.402.\n",
      "At iteration 70 the loss is 4.303.\n",
      "At iteration 80 the loss is 11.817.\n",
      "At iteration 90 the loss is 14.429.\n",
      "At iteration 100 the loss is 11.519.\n",
      "At iteration 110 the loss is 9.607.\n",
      "At iteration 120 the loss is 20.591.\n",
      "At iteration 130 the loss is 8.957.\n",
      "Epoch: 28, time taken: 4.5s, validation accuracy: 0.756, recall: 0.3278711438179016, precision: 0.6865546107292175.\n",
      "At iteration 10 the loss is 16.784.\n",
      "At iteration 20 the loss is 9.064.\n",
      "At iteration 30 the loss is 9.373.\n",
      "At iteration 40 the loss is 11.148.\n",
      "At iteration 50 the loss is 6.666.\n",
      "At iteration 60 the loss is 17.367.\n",
      "At iteration 70 the loss is 11.966.\n",
      "At iteration 80 the loss is 8.744.\n",
      "At iteration 90 the loss is 7.037.\n",
      "At iteration 100 the loss is 8.745.\n",
      "At iteration 110 the loss is 22.609.\n",
      "At iteration 120 the loss is 15.130.\n",
      "At iteration 130 the loss is 11.966.\n",
      "Epoch: 29, time taken: 4.0s, validation accuracy: 0.756, recall: 0.3278711438179016, precision: 0.6865546107292175.\n",
      "At iteration 10 the loss is 6.587.\n",
      "At iteration 20 the loss is 22.610.\n",
      "At iteration 30 the loss is 6.665.\n",
      "At iteration 40 the loss is 8.851.\n",
      "At iteration 50 the loss is 14.673.\n",
      "At iteration 60 the loss is 4.188.\n",
      "At iteration 70 the loss is 6.940.\n",
      "At iteration 80 the loss is 8.957.\n",
      "At iteration 90 the loss is 14.052.\n",
      "At iteration 100 the loss is 17.364.\n",
      "At iteration 110 the loss is 11.818.\n",
      "At iteration 120 the loss is 8.953.\n",
      "At iteration 130 the loss is 14.276.\n",
      "Epoch: 30, time taken: 4.3s, validation accuracy: 0.756, recall: 0.3376750648021698, precision: 0.6865546107292175.\n",
      "The estimated test accuracy is 0.744, recall 0.31848740577697754, precision 0.7254902124404907.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "EPOCHS = 30 # epoch\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "accuracies=[]\n",
    "recalls=[]\n",
    "precisions=[]\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_an_epoch(train_dataloader, optimizer)\n",
    "    accuracy, recall, precision = get_accuracy(valid_dataloader)\n",
    "    accuracies.append(accuracy)\n",
    "    recalls.append(recall)\n",
    "    precisions.append(precision)\n",
    "    time_taken = time.time() - epoch_start_time\n",
    "    print(f'Epoch: {epoch}, time taken: {time_taken:.1f}s, validation accuracy: {accuracy:.3f}, recall: {recall}, precision: {precision}.')\n",
    "    \n",
    "plt.plot(range(1, EPOCHS+1), accuracies)\n",
    "\n",
    "accuracy, recall, precision = get_accuracy(test_dataloader)\n",
    "print(f'The estimated test accuracy is {accuracy:.3f}, recall {recall}, precision {precision}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-project-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
